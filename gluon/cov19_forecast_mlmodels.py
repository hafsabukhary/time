# -*- coding: utf-8 -*-
"""COV19-forecast_mlmodels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jj0460ZGHDOaujyT49lWci2kFrdxWkkL
"""

#### Install MLMODELS in Colab

 
!  bash <(wget -qO- https://cutt.ly/mlmodels)  &> log.txt
!pip install pydantic==1.4 --force
 
import os, time
os.kill(os.getpid(), 9)

from google.colab import drive
drive.mount('/content/drive')

!pip install mxnet 
!pip install gluonts

!ls drive

###############  Working directory  ######################################################
import os
dirdrive = "/content/drive/My Drive/Colab Notebooks/shared_session/timeseries_example"
os.chdir(dirdrive)



# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import mxnet as mx
from mxnet import gluon
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import os
from tqdm.autonotebook import tqdm
from pathlib import Path

csv_data= "kaggle_data/covid19_forecast" 
gluonts_data= "gluonts_data/covid1"



"""
Manual Pre-processing

"""

train_df = pd.read_csv(csv_data+"/train.csv", index_col=False)
test     = pd.read_csv(csv_data+"/test.csv", index_col=False)
print("original data")
print(train_df.head(5))
print(test.head(5))


#### PARAMS ####################################################################
freq = "D"
prediction_length = 20
start="2020-01-23 00:00:00"

#### Cardinality count #########################################################
"""
stat_cat_features  = []
#         country with small number of people
#         country with median number of people
#         country with large number of people

"""
cat_cardinality = [3]    

print("cardinalities")
print(cat_cardinality)




#### Feature creation #########################################################
train_df = train_df[train_df["Target"]=="ConfirmedCases"]
test     = test[test["Target"]=="ConfirmedCases"]
train_df = train_df.fillna("")


train_df["name"] = train_df["Country_Region"] + "_" + train_df["Province_State"] + "_" + train_df["County"]


country_list = sorted(list(set(train_df["name"])))
date_list    = sorted(list(set(train_df["Date"])))
data_dic     = {"name": country_list}

for date in date_list:
    tmp            = train_df[train_df["Date"]==date][["name", "Date", "TargetValue"]]
    tmp            = tmp.pivot(index="name", columns="Date", values="TargetValue")
    tmp_values     = tmp[date].values
    data_dic[date] = tmp_values
new_df = pd.DataFrame(data_dic) #df



feature_dic_population = {}
feature_dic_weight = {}
for date in date_list:
    tmp                          = train_df[train_df["Date"]==date][["Date", "name", "Population", "Weight"]]
    population                   = tmp.pivot(index="name", columns="Date", values="Population")
    weight                       = tmp.pivot(index="name", columns="Date", values="Weight")
    feature_dic_population[date] = population[date].values
    feature_dic_weight[date]     = weight[date].values

feature_df_population = pd.DataFrame(feature_dic_population) #df
feature_df_weight     = pd.DataFrame(feature_dic_weight) #df



## date is col name in both df. 
## make a dict to hold col. nan value is empty space
train_df = new_df.drop(["name"], axis=1)

populations = []
weights = []
for i in range(feature_df_population.shape[0]):
    populations.append(feature_df_population.values[i][0])
    weights.append(feature_df_weight.values[i][0])
print(weights[:][0:5])
def min_max_scale(lst):
    minimum = min(lst)
    maximum = max(lst)
    new = []
    for i in range(len(lst)):
        new.append((lst[i] - minimum) / (maximum - minimum))
    return new


scaled_populations = min_max_scale(populations)
scaled_weights     = min_max_scale(weights)
print(scaled_weights)
start_date = ["2020-01-23 00:00:00" for _ in range(len(new_df))]
print(feature_df_weight.head(10))

static_df = pd.DataFrame(data=scaled_weights)
############# ENCODE DATAFRAMES###############################
print("df_timeseries :: train_df")
print(train_df.head())
df_timeseries_dtype = {}
df_timeseries_cols = {}
k = 0
for col in train_df.columns:
    df_timeseries_dtype[col] = str(train_df[col].dtype)
    df_timeseries_cols[k] = col
    k = k +1

print("static real df")
print(static_df.head(10))
df_static_real_dtype = {}
k = 0 
for col in static_df.columns:
  df_static_real_dtype[col] = str(static_df[col].dtype)
  k = k +1


######### Cum Sum of time series #####################################
total_values = train_df.values
row, col = total_values.shape
for i in range(row):
    tmp = total_values[i]
    for j in range(col):
        if j > 0:
            tmp[j] = tmp[j] + tmp[j - 1]
    total_values[i] = tmp


# feature_df_population.head()


######## Static Features ############################################
stat_real_features = []
stat_cat_features  = []

for i in range(len(static_df)):
    if 0 <= static_df[0][i] <= 0.33:
#         country with small number of people
        stat_cat_features.append([1])
    elif 0.33 < static_df[0][i] <= 0.67:
#         country with median number of people
        stat_cat_features.append([2])
    else:
#         country with large number of people
        stat_cat_features.append([3])
    stat_real_features.append([static_df[0][i]])

#### Match with Functions



df_timeries

from util import (
    
gluonts_save_to_file, gluonts_create_dataset

)





from gluonts.dataset.common import load_datasets, ListDataset
from gluonts.dataset.field_names import FieldName


def gluonts_create_dataset(timeseries_list, start_dates_list, feat_dynamic_list=None,  feat_static_list=None, feat_static_real_list=None, freq="D" ) :
    feat_dynamic = True
    feat_static = True
    feat_static_real = True
    k = len(start_dates_list)
    train_ds = []
    if feat_dynamic_list is None :     feat_dynamic_list, feat_dynamic     = [ [] ]  * k  , False
    if feat_static_list is None :      feat_static_list, feat_static       = [ [] ]  * k  , False
    if feat_static_real_list is None : feat_static_real_list, feat_static_real = [ [] ]  * k  , False

    for (target, start, fdr, fsr, fsc ) in zip(timeseries_list,               # List of individual time series
                                               start_dates_list,        # list of start dates
                                               feat_dynamic_list,       # List of Dynamic Features
                                               feat_static_real_list,   # List of static real Features                                         
                                               feat_static_list)  :     # List of Static Features     
 
      xi = {}
      xi[FieldName.TARGET]  = target.tolist()
      xi[FieldName.START ]  = start
      if feat_dynamic     : xi[FieldName.FEAT_DYNAMIC_REAL] = fdr.tolist()
      if feat_static_real : xi[FieldName.FEAT_STATIC_REAL]  = fsr
      if feat_static      : xi[FieldName.FEAT_STATIC_CAT]  = fsc
      train_ds.append(xi)

    return train_ds

"""
    train_ds = [
        {
            FieldName.TARGET            : target.tolist(),
            FieldName.START             : start,
            FieldName.FEAT_DYNAMIC_REAL : fdr.tolist(),
            FieldName.FEAT_STATIC_REAL: : fsr.tolist() if feat_dynamic_list is not None else [] ,
            FieldName.FEAT_STATIC_CAT   : fsc.tolist() if feat_dynamic_list is not None else [] 
        } 
        ]
    return train_ds
"""

from typing import Dict, List
def to_gluonts_json(path:Path, data: List[Dict]):
    import os
    print(f"saving time-series into {path}")
    path=os.path.join(path ,"dataa.json")
    path_dir = os.path.dirname(path)
    os.makedirs(path_dir, exist_ok=True)
    with open(path, 'wb') as fp:
        for d in data:
            fp.write(json.dumps(d).encode("utf-8"))
            fp.write("\n".encode('utf-8'))

### From csv to gluonts
from gluonts.dataset.common import load_datasets, ListDataset
from gluonts.dataset.field_names import FieldName


import json

train_file = gluonts_data+"/train/data.json"
test_file =  gluonts_data+"/test/data.json"
meta_file =  gluonts_data+'/metadata.json' 

#### Output JSON  ##############################################################
with open(meta_file, 'w') as f:
      f.write(  json.dumps(
      {"cardinality":cat_cardinality,
       "freq":"D",
       "prediction_length":prediction_length,        
               }
              )
      )
      
      
### Meta data for decoding df ############################################3#####
with open(gluonts_data+'/decode.json', 'w') as f:
          f.write(
              json.dumps({
                 "df_dynamic_cols":None, "df_dynamic_labels":None, "df_static_cols":None, 
                 "df_static_labels":None, "df_timeseries_cols":df_timeseries_cols,"df_timeseries_dtype":df_timeseries_dtype,
                 "df_dynamic_dtype":None,"df_static_dtype":None ,"df_static_real_cols":None,"df_static_real_dtype":df_static_real_dtype ,
                 }
                  )
              )  


#### Output JSON  ##############################################################
train_target_values = [ts[:-prediction_length] for ts in total_values]


"""
train_ds = [
    {
        FieldName.TARGET: target.tolist(),
        FieldName.START: start,
        FieldName.FEAT_STATIC_REAL: static_real,
        FieldName.FEAT_STATIC_CAT:  static_cat
    }
    for (target, start,static_real, static_cat) in zip(train_target_values,
                                         start_date,
                                         stat_real_features,
                                         stat_cat_features)
]
to_gluonts_json(train_file, train_ds)
"""
test_target_values = total_values.copy()
train_target_values = [ts[:-prediction_length] for ts in total_values]



train_ds = gluonts_create_dataset(train_target_values, start_date, feat_dynamic_list=None,  feat_static_list=stat_cat_features, feat_static_real_list=stat_real_features, freq="D" ) 


to_gluonts_json(train_file, train_ds)




# Build testing set
#### Output JSON  ##############################################################
test_target_values = total_values.copy()
"""
test_ds = [
    {
       
        FieldName.TARGET: target.tolist(),
        FieldName.START: start,
        FieldName.FEAT_STATIC_REAL: static_real,
        FieldName.FEAT_STATIC_CAT: static_cat
    }
    for (target, start, static_real,  static_cat) in zip(test_target_values,
                                         start_date,
                                        stat_real_features, 
                                        stat_cat_features)
    
]
"""
test_ds  = gluonts_create_dataset(test_target_values, start_date, feat_dynamic_list=None,  feat_static_list=stat_cat_features, feat_static_real_list=stat_real_features, freq="D" ) 
to_gluonts_json(test_file, test_ds)

def gluonts_to_pandas(dataset_path, data_type):
  from gluonts.dataset.common import ListDataset,load_datasets  
  from copy import deepcopy
  all_targets = []
  all_dynamic = []
  all_static  = []
  all_static_Real = []
  start       = []
  TD          = load_datasets(  metadata=dataset_path,
                                  train=dataset_path / "train", test=dataset_path / "test")
  
  ### json iterator  Why Test only   ###############
  if data_type == "test"  : TD_current = deepcopy( TD.test )
  if data_type == "train" : TD_current = deepcopy( TD.train )


  instance_iter =next(iter(TD_current))
  df_static_real = None
  df_dynamic = None
  df_static = None
  df_static_real = None

  #### load decode pars ############################
  decode_pars = json.load(open(dataset_path / "decode.json", mode='r'),object_hook=lambda d: {int(k) 
                          if k.lstrip('-').isdigit() else k: v for k, v in d.items()})
  print(decode_pars)
  df_dynamic_labels   = decode_pars["df_dynamic_labels"]
  df_dynamic_cols     = decode_pars["df_dynamic_cols"]
  df_static_labels    = decode_pars["df_static_labels"]
  df_static_cols      = decode_pars["df_static_cols"]
  df_static_real_cols = decode_pars["df_static_real_cols"]

  df_timeseries_cols  = decode_pars["df_timeseries_cols"]
  df_timeseries_dtype = decode_pars["df_timeseries_dtype"]
  df_dynamic_dtype    = decode_pars["df_dynamic_dtype"]
  df_static_dtype     = decode_pars["df_static_dtype"]
  df_static_real_dtype =decode_pars["df_static_real_dtype"]

  #################################################
  if "feat_dynamic_real" in instance_iter:
    dynamic_features = np.transpose(instance_iter["feat_dynamic_real"])
  else:
    dynamic_features = None


  for items in TD_current :
    #print(items)
    target=np.transpose(items["target"]).tolist() 
    if "feat_static_cat" in items:
      static= np.transpose(items["feat_static_cat"]).tolist()

    if "feat_static_real" in items:
      static_real = items["feat_static_real"]
      all_static_Real.append(static_real)
    
    all_static.append(static)
    all_targets.append(target)

  

  df_timeseries =pd.DataFrame(all_targets)
  del all_targets

  ############# decode df_static real ############################
  if len(all_static_Real)>0:
    df_static_real=pd.DataFrame(all_static_Real)
    df_static_real     = df_static_real.astype(df_static_real_dtype) 

  ################ decode  df_dynamic #####   
  if dynamic_features:
    df_dynamic =pd.DataFrame(dynamic_features)
    if  df_dynamic_labels is not None:
      for key in  df_dynamic_labels:
        col = key
        labels= df_dynamic_labels[key]
        for l in labels:
          v = labels[l]
          df_dynamic[col]=df_dynamic[col].apply(lambda x: v if x == l else x)

    for col in df_dynamic.columns:       
      df_dynamic[col]=df_dynamic[col].apply(lambda x: np.NAN if x == -l else x)  
    
    if  df_dynamic_cols is not None:
      df_dynamic.rename(columns =  df_dynamic_cols, inplace = True) 
    df_dynamic    = df_dynamic.astype(df_dynamic_dtype)
  
  if len(all_static)>0:  
    df_static =pd.DataFrame(all_static)


  ##### decode df_timeseries#############
  if  df_timeseries_cols is not None:
    df_timeseries.rename(columns = df_timeseries_cols , inplace = True) 
  df_timeseries = df_timeseries.astype(df_timeseries_dtype)
  

  ####### decode df staic################
  if not df_static.empty:
    if df_static_labels  is not None:
        for key in df_static_labels: 
          d =  df_static_labels[key]
          df_static[key] = df_static[key].map(d)
    if  df_static_cols is not None:
        df_static.rename(columns = df_static_cols, inplace = True) 
    df_static     = df_static.astype(df_static_dtype)    
 

  ########## df static real ###########

  
  
  #####################################
  return df_timeseries,df_dynamic,df_static,df_static_real



def pd_difference(df1, df2,is_real=False):
  """Identify differences between two pandas DataFrames
    
  """
  if (df1.columns != df2.columns).any(0):
    print("DataFrame column names are different")
    return None
 
  if any(df1.dtypes != df2.dtypes):
        print("Data Types are different, trying to convert")
        df2 = df2.astype(df1.dtypes)
        
  if df1.equals(df2):
        print("Exactly Same")
        return None
  else:
        df1=df1.fillna(-1)
        df2=df2.fillna(-1)
        if is_real:
          if not (np.abs(df1 - df2) >9.62340710e2).any().values:
            print("Real Dataframes are equal")
            return None 
          else: 
            diff_mask=(np.abs(df1 - df2) >9.62340710e2)
        else:
          # need to account for np.nan != np.nan returning True
          diff_mask = (df1 != df2) & ~(df1.isnull() & df2.isnull())
        ne_stacked = diff_mask.stack()
        changed = ne_stacked[ne_stacked]
        changed.index.names = ['id', 'col']
        difference_locations = np.where(diff_mask)
        changed_from = df1.values[difference_locations]
        changed_to = df2.values[difference_locations]
        return pd.DataFrame({'from': changed_from, 'to': changed_to},
                            index=changed.index)


def gluonts_json_check() :
  ##### Check Code
  from pathlib import Path

  dataset_path = Path(gluonts_data)
  df_timeseries1,df_dynamic1,df_static1,df_static_real1 = gluonts_to_pandas(dataset_path, data_type="test")
  #print('###sum of df_static_real ####' )
  #print(static_df-df_static_real1.sum())
  print(df_timeseries1.shape,df_static_real1.shape  )

  print('### matching  df_timeseries ####' )
  print(pd_difference(train_df,df_timeseries1))

  print('### matching  df_static_real ####' )
  print(pd_difference(static_df,df_static_real1,is_real=True))


gluonts_json_check()

for col in df_static_real1.columns:
  print(df_static_real1[col].dtype)
  
for col in static_df.columns:
  print(static_df[col].dtype)

df_static1.dtypes[0]

static_df

from gluonts.dataset.common import ListDataset,load_datasets
### dataset loaded once wil be deleted
dataset_path  =Path(gluonts_data)
TD            =load_datasets(    metadata=dataset_path,
                   train=dataset_path / "train",
                   test=dataset_path / "test",)
print(TD)

print(next(iter(TD.train)))



















# import library
import mlmodels
from mlmodels.models import module_load
from mlmodels.util import path_norm_dict, path_norm, params_json_load
import json


json_path="covid19_gluonts.json"
#### Model URI and Config JSON
#model_uri   = "model_keras.gluonts_model"
#config_path = path_norm( 'model_keras/ardmn.json'  )
#config_mode = "test"  ### test/prod
model_uri="gluonts_model"

"""

###########################################
SAMPLE JSON 
##########################################
"""
#### Load JSON File (no python is included)
## DO NOT INCLUDE PYTHON variables, only string or json values

jpars={
    "deepar": {
         "model_pars": {
             "model_uri"  : "model_gluon.gluonts_model",
             "model_name" : "deepar",
             "model_pars" : {
                 "prediction_length": 20, 
                 "freq": "D",
                 "distr_output" :  "gluonts.distribution.neg_binomial:NegativeBinomialOutput", 
                
               
                 "use_feat_static_real":true,
                 
                  "num_layers": 2, 
                  "num_cells": 40, 
                  "cell_type": "lstm", 
                  "dropout_rate": 0.1,

                 
                  "scaling": true, 
                  
                  "num_parallel_samples": 100,
                  "cardinality":  "None"
             }
             
             
             },
        "data_pars": {
            "train": true, 
            "dt_source": "",
            "data_type":"gluonts",

                
              
            "use_feat_static_real":true,

           
            "submission": false , 
          
            
            "data_path":  "gluonts_data/covid19_forecast" 
           
           
            "num_series" : 1
            
                    
            },
            
            
        "compute_pars": {
            "num_samples": 100,
            
            "learning_rate"         : 1e-3,
            "epochs"                : 100,
            "num_batches_per_epoch" : 50,
            "batch_size"            : 32,   
            
            "compute_pars" : {
                
                "batch_size": 32, "clip_gradient": 100, "epochs": 50, "init": "xavier", "learning_rate": 1e-5, 
                "learning_rate_decay_factor": 0.5, 
                "hybridize": false,
                "num_batches_per_epoch": 50,
                
                "minimum_learning_rate": 5e-05, "patience": 10, "weight_decay": 1e-08
            }
        },
        
      "out_pars": {
         "path": "ztest/model_gluon/gluonts_deepar/",
         "plot_prob": true, "quantiles": [0.5]
      }
    }
}

json.dump(jpars,  open(json_path, mode="w") )

import imp
#import gluonts_model as module
imp.reload(module)

jpars=None

#### Model Parameters
jpars = json.load(open(json_path))

for x in [ 'model_pars', 'data_pars', 'compute_pars', 'out_pars' ] :
  globals()[x] = jpars['deepar'][x]
print( model_pars, data_pars, compute_pars, out_pars)

#### Setup Model 
#module         = module_load( model_uri)
#model          = module.Model(model_pars, data_pars, compute_pars) 
module=None
import gluonts_model as module

model = module.Model(model_pars=model_pars, data_pars=data_pars, compute_pars=compute_pars)

model = module.fit(model=model,data_pars= data_pars,compute_pars= compute_pars, out_pars=out_pars) 
model



#### Fit          >>>>USE module.fit

#model = module.fit(model=model,data_pars= data_pars,compute_pars= compute_pars, out_pars=out_pars)           #### fit model
y_pred=module.predict(model=model,data_pars= data_pars,compute_pars= compute_pars, out_pars=out_pars) 
 
metrics_val    = module.fit_metrics(y_pred, data_pars, compute_pars, out_pars)   #### Check fit metrics
print(metrics_val)

module.plot_prob_forecasts(y_pred)







#### Save/Load
module.save(model, save_pars ={ 'path': out_pars['path'] +"/model/"})

model2 = module.load(load_pars ={ 'path': out_pars['path'] +"/model/"})

from gluonts.model.deepar import DeepAREstimator
from gluonts.distribution.neg_binomial import NegativeBinomialOutput
from gluonts.trainer import Trainer
prediction_length=20
n = 50
estimator = DeepAREstimator(
    prediction_length=prediction_length,
    freq="D",
    distr_output = NegativeBinomialOutput(),
    use_feat_static_real=True,
#     use_feat_static_cat=True,
#     cardinality=cat_cardinality,
    trainer=Trainer(
        learning_rate=1e-5,
        epochs=n,
        num_batches_per_epoch=50,
        batch_size=32
    )
)
train_ds=TD.train
test_ds=TD.test
predictor = estimator.train(train_ds)

estimator

from gluonts.evaluation.backtest import make_evaluation_predictions

forecast_it, ts_it = make_evaluation_predictions(
    dataset=test_ds,
    predictor=predictor,
    num_samples=100
)

print("Obtaining time series conditioning values ...")
tss = list(tqdm(ts_it, total=len(test_ds)))
print("Obtaining time series predictions ...")
forecasts = list(tqdm(forecast_it, total=len(test_ds)))

from gluonts.evaluation import Evaluator


class CustomEvaluator(Evaluator):

    def get_metrics_per_ts(self, time_series, forecast):
        successive_diff = np.diff(time_series.values.reshape(len(time_series)))
        successive_diff = successive_diff ** 2
        successive_diff = successive_diff[:-prediction_length]
        denom = np.mean(successive_diff)
        pred_values = forecast.samples.mean(axis=0)
        true_values = time_series.values.reshape(len(time_series))[-prediction_length:]
        num = np.mean((pred_values - true_values) ** 2)
        rmsse = num / denom
        metrics = super().get_metrics_per_ts(time_series, forecast)
        metrics["RMSSE"] = rmsse
        return metrics

    def get_aggregate_metrics(self, metric_per_ts):
        wrmsse = metric_per_ts["RMSSE"].mean()
        agg_metric, _ = super().get_aggregate_metrics(metric_per_ts)
        agg_metric["MRMSSE"] = wrmsse
        return agg_metric, metric_per_ts


evaluator = CustomEvaluator(quantiles=[0.5, 0.67, 0.95, 0.99])
agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))
print(json.dumps(agg_metrics, indent=4))