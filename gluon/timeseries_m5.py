# -*- coding: utf-8 -*-
"""timeseries_m5_mlmodels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D9YQR8lISjSNQnjd2cSqzH7waay_ITj-
"""

#### Install MLMODELS in Colab


"""
!wget -qO- https://cutt.ly/mlmodels_dev2
!pip install pydantic==1.4 --force
!chmod +x /content/run_install.sh
!/content/run_install.sh
!rm -f run_install.sh.*
!ls
### Restart Runtime AFTER Install
import os, time
os.kill(os.getpid(), 9)
"""




!  bash <(wget -qO- https://cutt.ly/mlmodels)
!pip install pydantic==1.4 --force

import os, time
os.kill(os.getpid(), 9)

from google.colab import drive
drive.mount('/content/drive')

from mlmodels import util
print(util)

#Working directory
import os
os.chdir("/content/drive/My Drive/Colab Notebooks/shared_session/timeseries_example")

dirdrive = "/content/drive/My Drive/Colab Notebooks/shared_session/timeseries_example"

pip install mxnet

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import mxnet as mx
from mxnet import gluon
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import os
from tqdm.autonotebook import tqdm
from pathlib import Path



########################
single_pred_length = 28
submission_prediction_length = single_pred_length * 2

submission=False

if submission:
    prediction_length = submission_pred_length
else:
    prediction_length = single_pred_length

prediction_length

#load data  from SHARED FOLDER
data_folder = "kaggle_data/m5_dataset"
gluonts_datafolder='gluonts_data/m5_dataset'

calendar               = pd.read_csv(data_folder+'/calendar.csv')
sales_train_val        = pd.read_csv(data_folder+'/sales_train_validation.csv.zip')


sample_submission      = pd.read_csv(data_folder+'/sample_submission.csv.zip')
sell_prices            = pd.read_csv(data_folder+'/sell_prices.csv.zip')

#### https://github.com/awslabs/gluon-ts/blob/master/src/gluonts/dataset/repository/_m5.py
"""
(  train_target_values, dates, train_cal_features_list, stat_cat  )

"""
# Build dynamic cols
cal_cols = calendar.drop( ["date", "wm_yr_wk", "weekday", "wday", "month", "year", "event_name_1", "event_name_2", "d", ],  axis=1,)
cal_cols["event_type_1"] = cal_cols["event_type_1"].apply( lambda x: 0 if str(x) == "nan" else 1)
cal_cols["event_type_2"] = cal_cols["event_type_2"].apply( lambda x: 0 if str(x) == "nan" else 1)


test_cal_cols  = cal_cols.values.T
train_cal_cols = test_cal_cols[ :, : -submission_prediction_length - prediction_length]
train_cal_cols_list = [train_cal_cols] * len( sales_train_val)


test_cal_cols  = test_cal_cols[:, :-submission_prediction_length]
test_cal_cols_list = [test_cal_cols] * len(sales_train_val)




# Build static cols
state_ids = ( sales_train_val["state_id"].astype("category").cat.codes.values)
state_ids_un = np.unique(state_ids)


store_ids = ( sales_train_val["store_id"].astype("category").cat.codes.values)
store_ids_un = np.unique(store_ids)


cat_ids = (sales_train_val["cat_id"].astype("category").cat.codes.values)
cat_ids_un = np.unique(cat_ids)

dept_ids = (  sales_train_val["dept_id"].astype("category").cat.codes.values)
dept_ids_un = np.unique(dept_ids)

item_ids = ( sales_train_val["item_id"].astype("category").cat.codes.values)
item_ids_un = np.unique(item_ids)

stat_cat_list = [item_ids, dept_ids, cat_ids, store_ids, state_ids]
stat_cat = np.concatenate(stat_cat_list)
stat_cat = stat_cat.reshape(len(stat_cat_list), len(item_ids)).T

#### Mdel definiton
cardinalities = [
    len(item_ids_un),
    len(dept_ids_un),
    len(cat_ids_un),
    len(store_ids_un),
    len(state_ids_un),
]



##### Params 1:  Target Values to Predict
train_df            = sales_train_val.drop( ["id", "item_id", "dept_id", "cat_id", "store_id", "state_id"], axis=1)
train_target_values = [ts[:-prediction_length] for ts in train_df.values]


##### Params 2: Date start Params
dates = ["2011-01-29 00:00:00" for _ in range(len(sales_train_val))]


#### Test Data
test_target_values  = train_df.values.copy()

def save_to_file(path="", data=None):
    import os
    print(f"saving time-series into {path}")
    path_dir = os.path.dirname(path)
    os.makedirs(path_dir, exist_ok=True)
    with open(path, 'wb') as fp:
        for d in data:
            fp.write(json.dumps(d).encode("utf-8"))
            fp.write("\n".encode('utf-8'))











import json

with open(gluonts_datafolder+'/metadata.json', 'w') as f:
      d =  {"cardinality":cardinalities,
                "freq":"D",
                "prediction_length":prediction_length,

               }
      f.write(json.dumps(d)
          )
      

      
train_file  = gluonts_datafolder+"/train/data.json"
test_file =   gluonts_datafolder+"/test/data.json"

from gluonts.dataset.field_names import FieldName

train_ds = [  {   FieldName.TARGET: target.tolist(),
                  FieldName.START: start,
                  FieldName.FEAT_DYNAMIC_REAL: fdr.tolist(),
                 FieldName.FEAT_STATIC_CAT: fsc.tolist(),
    }
    for (target, start, fdr, fsc) in zip(  train_target_values, dates, train_cal_features_list, stat_cat  )
]
save_to_file(train_file, train_ds)


################# Build testing set
test_ds = [  { FieldName.TARGET: target.tolist(),
               FieldName.START: start,
               FieldName.FEAT_DYNAMIC_REAL: fdr.tolist(),
               FieldName.FEAT_STATIC_CAT: fsc.tolist(),
    }
    for (target, start, fdr, fsc) in zip(test_target_values, dates, test_cal_features_list, stat_cat)
]
save_to_file(test_file, test_ds)

del train_ds,test_ds

#@title

df_dynamic.to_csv(data_folder+'/df_dynamic.csv',index=False)
df_static.to_csv(data_folder+'/df_static.csv',index=False)
df_timeseries.to_csv(data_folder+'/df_timeseries.csv',index=False)

#@title
df_dynamic    = pd.read_csv(data_folder+'/df_dynamic.csv')
df_static     = pd.read_csv(data_folder+'/df_static.csv')
df_timeseries = pd.read_csv(data_folder+'/df_timeseries.csv')

#@title
df_timseries.head(5)

"""
########################
SAMPLE JASON FOR M5
#######################
false=False
true=True

jpars={
    "deepar": {
         "model_pars": {
             "model_uri"  : "model_gluon.gluonts_model",
             "model_name" : "deepar",
             "model_pars" : {
                 "prediction_length": 12, 
                 "freq": "D",
                 "distr_output" :  "gluonts.distribution.neg_binomial:NegativeBinomialOutput", 
                

                 "use_feat_dynamic_real":true, 
                 "use_feat_static_cat": true, 
                 "use_feat_static_real":false,
                 
                  "num_layers": 2, 
                  "num_cells": 40, 
                  "cell_type": "lstm", 
                  "dropout_rate": 0.1,

                 
                  "scaling": false, 
                  "num_parallel_samples": 100,
                  "cardinality": [3049, 7, 3, 10, 3]
             }
             
             
             },
        "data_pars": {
            "train": true, 
            "dt_source": "",


                 "use_feat_dynamic_real":true, 
                 "use_feat_static_cat": true, 
                 "use_feat_static_real":false,

           
            "submission": false , 
            
            "data_path":  "gluonts_data/m5_dataset" , 
           
            "single_pred_length":28,
            "freq": "1D",
            "start" : "2015-02-26 21:42:53",        
            "startdate" : "2011-01-29",  
            "col_date"   : "timestamp",                
            "col_ytarget" : ["value"],  "num_series" : 1,

            "cols_cat": [],   "cols_num" : []
                    
            },
            
            
        "compute_pars": {
            "num_samples": 100,
            
            "learning_rate"         : 1e-3,
            "epochs"                : 100,
            "num_batches_per_epoch" : 50,
            "batch_size"            : 32,   
            
            "compute_pars" : {
                
                "batch_size": 32, "clip_gradient": 100, "epochs": 1, "init": "xavier", "learning_rate": 1e-3, 
                "learning_rate_decay_factor": 0.5, 
                "hybridize": false,
                "num_batches_per_epoch": 10,
                
                "minimum_learning_rate": 5e-05, "patience": 10, "weight_decay": 1e-08
            }
        },
        
      "out_pars": {
         "path": "ztest/model_gluon/gluonts_deepar/",
         "plot_prob": true, "quantiles": [0.5]
      }
    }
}
"""



# from gluonts.dataset.common import load_datasets, ListDataset
# from gluonts.dataset.field_names import FieldName









import imp
imp.reload(module)

"""

https://github.com/arita37/mlmodels/blob/dev/mlmodels/model_gluon/gluonts_model.py



"""
# import library
import mlmodels
from mlmodels.models import module_load
from mlmodels.util import path_norm_dict, path_norm, params_json_load
import json



#### Model URI and Config JSON
#model_uri   = "model_keras.gluonts_model"
#config_path = path_norm( 'model_keras/ardmn.json'  )
#config_mode = "test"  ### test/prod
model_uri="gluonts_model"
module=None


#### Model Parameters
jpars = json.load(open("m5_gluonts.json", mode='r'))
print(jpars)

for x in [ 'model_pars', 'data_pars', 'compute_pars', 'out_pars' ] :
  globals()[x] = jpars['deepar'][x]
print( model_pars, data_pars, compute_pars, out_pars)

test_ds=None
train_ds=None
# test gluonts data
from gluonts.dataset.common import ListDataset,load_datasets
dataset_path  =Path(data_pars['data_path'])
    
TD  =load_datasets( metadata=dataset_path,
                    train= dataset_path / "train",  test= dataset_path / "test",)

#### Setup Model 

#module         = module_load( model_uri)
#model          = module.Model(model_pars, data_pars, compute_pars) 
model=None
module=None
import gluonts_model as module

model=module.Model(model_pars=model_pars, data_pars=data_pars, compute_pars=compute_pars)

len(TD.test)



#### Fit          >>>>USE module.fit

model = module.fit(model=model,data_pars= data_pars,compute_pars= compute_pars, out_pars=out_pars)           #### fit model
y_pred=module.predict(model=model,data_pars= data_pars,compute_pars= compute_pars, out_pars=out_pars)  
metrics_val    = module.fit_metrics(y_pred, data_pars, compute_pars, out_pars)   #### Check fit metrics
print(metrics_val)









#### Save/Load
module.save(model, save_pars ={ 'path': out_pars['path'] +"/model/"})

model2 = module.load(load_pars ={ 'path': out_pars['path'] +"/model/"})

from gluonts.model.deepar import DeepAREstimator
from gluonts.distribution.neg_binomial import NegativeBinomialOutput
from gluonts.trainer import Trainer

estimator = DeepAREstimator(
    prediction_length     = 12,
    freq                  = "D",
    distr_output          = NegativeBinomialOutput(),
    use_feat_static_cat   =True,
    use_feat_dynamic_real =True,
    cardinality           = [3049, 7, 3, 10, 3],
    trainer               = Trainer(
    learning_rate         = 1e-3,
    epochs                = 1,
    num_batches_per_epoch = 10,
    batch_size            = 10
    )
)

predictor = estimator.train(TD.train)

from gluonts.evaluation.backtest import make_evaluation_predictions
from gluonts.evaluation import Evaluator

forecast_it, ts_it = make_evaluation_predictions(
    dataset=TD.test,
    predictor=predictor,
    num_samples=100
)



from gluonts.evaluation import Evaluator
agg_metrics, item_metrics = Evaluator()(ts_it, forecast_it, num_series=len(TD.test) )
print(agg_metrics)

"""
Converting forecasts back to M5 submission format (if submission is True)
Since GluonTS estimators return a sample-based probabilistic forecasting predictor, we first need to reduce these results to a single pred per time series. This can be done by computing the mean or median over the predicted sample paths.
"""
########################
if submission == True:
    forecasts_acc = np.zeros((len(forecasts), pred_length))
    for i in range(len(forecasts)):
        forecasts_acc[i] = np.mean(forecasts[i].samples, axis=0)


# We then reshape the forecasts into the correct data shape for submission ...
########################
if submission == True:
    forecasts_acc_sub = np.zeros((len(forecasts)*2, single_pred_length))
    forecasts_acc_sub[:len(forecasts)] = forecasts_acc[:,:single_pred_length]
    forecasts_acc_sub[len(forecasts):] = forecasts_acc[:,single_pred_length:]

"""
.. and verfiy that reshaping is consistent.
"""
########################
if submission == True:
    np.all(np.equal(forecasts_acc[0], np.append(forecasts_acc_sub[0], forecasts_acc_sub[30490])))


## Then, we save our submission into a timestamped CSV file which can subsequently be uploaded to Kaggle.
########################
if submission == True:
    import time
    sample_submission            = pd.read_csv(data_folder/sample_submission.csv')
    sample_submission.iloc[:,1:] = forecasts_acc_sub
    submission_id                = 'submission_{}.csv'.format(int(time.time()))
    sample_submission.to_csv(submission_id, index=False)

plot_log_path = "shared_session/timeseries_m5_benchmark/plots/"
directory = os.path.dirname(plot_log_path)
if not os.path.exists(directory):
    os.makedirs(directory)
    
def plot_prob_forecasts(ts_entry, forecast_entry, path, sample_id, inline=True):
    plot_length = 150
    prediction_intervals = (50, 67, 95, 99)
    legend = ["observations", "median prediction"] + [f"{k}% prediction interval" for k in prediction_intervals][::-1]

    _, ax = plt.subplots(1, 1, figsize=(10, 7))
    ts_entry[-plot_length:].plot(ax=ax)
    forecast_entry.plot(prediction_intervals=prediction_intervals, color='g')
    ax.axvline(ts_entry.index[-pred_length], color='r')
    plt.legend(legend, loc="upper left")
    plt.savefig('{}forecast_{}.pdf'.format(path, sample_id))

    if inline:
        plt.show()
        plt.clf()
  
    plt.close()

print("Plotting time series predictions ...")
for i in tqdm(range(5)):
    ts_entry = tss[i]
    forecast_entry = forecasts[i]
    plot_prob_forecasts(ts_entry, forecast_entry, plot_log_path, i)